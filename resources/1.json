"<title id=entity_0>\n  <span style='font-size:11.96px; text-indent:145.99px;'>Abstract</span>\n</title>\n<title id=entity_1>\n  <span style='font-size:14.35px; text-indent:125.60px;'>Vision Grid Transformer for Document Layout Analysis</span>\n</title>\n<title id=entity_2>\n  <span style='font-size:11.96px; text-indent:50.11px;'>1. Introduction</span>\n</title>\n<plain text id=entity_3>\n  <span style='font-size:9.96px; text-indent:320.82px;'>To better exploit both visual and textual information for</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>the DLA task, grid-based methods [45, 22, 47] cast text with</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>layout information into</span>\n  <span style='font-size:9.96px; text-indent:401.64px;'> 2</span>\n  <span style='font-size:9.96px; text-indent:409.29px;'>D</span>\n  <span style='font-size:9.96px; text-indent:417.52px;'> semantic representations (char-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>grid [23, 22] or sentence-grid [45, 47]) and combine them</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>with visual features, achieving good results in the DLA task.</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>Although grid-based methods equip with an additional tex-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>tual input of grid, only visual supervision is used for the</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>model training in the DLA task. Since there are no explicit</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>textual objectives to guide the linguistic modeling, we con-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>sider that the capability of semantic understanding is lim-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>ited in grid-based models, compared with the existing docu-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>ment pre-trained mo dels [20]. Therefore, how to effectively</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>model semantic features based on grid representations is a</span>\n</plain text>\n<plain text id=entity_4>\n  <span style='font-size:9.96px; text-indent:62.07px;'>Document pre-trained models and grid-based models</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>have proven to be very effective on various tasks in Docu-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>ment AI. However, for the document layout analysis (DLA)</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>task, existing document pre-trained models, even those pre-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>trained in a multi-modal fashion, usually rely on either</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>textual features or visual features. Grid-based models for</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>DLA are multi-modality but largely neglect the effect of</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>pre-training.</span>\n  <span style='font-size:9.96px; text-indent:110.50px;'>To fully leverage multi-modal information</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>and exploit pre-training techniques to learn better repre-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>sentation for DLA, in this paper, we present VGT, a two-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>stream Vision Grid Transformer, in which Grid Transformer</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>(GiT) is proposed and pre-trained for 2D token-level and</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>segment-level semantic understanding. Furthermore, a new</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>dataset named D</span>\n  <span style='font-size:6.97px; text-indent:122.58px;'>4</span>\n  <span style='font-size:9.96px; text-indent:127.05px;'>LA, which is so far the most diverse</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>and detailed manually-annotated benchmark for document</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>layout analysis, is curated and released. Experiment re-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>sults have illustrated that the proposed VGT model achieves</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>new state-of-the-art results on DLA tasks, e.g. PubLayNet</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>(</span>\n  <span style='font-size:9.96px; text-indent:53.43px;'>95</span>\n  <span style='font-size:9.96px; text-indent:63.39px;'>.</span>\n  <span style='font-size:9.96px; text-indent:66.16px;'>7%</span>\n  <span style='font-size:9.96px; text-indent:79.44px;'>\u2192</span>\n  <span style='font-size:9.96px; text-indent:89.41px;'>96</span>\n  <span style='font-size:9.96px; text-indent:99.37px;'>.</span>\n  <span style='font-size:9.96px; text-indent:102.14px;'>2%</span>\n  <span style='font-size:9.96px; text-indent:115.42px;'>), DocBank (</span>\n  <span style='font-size:9.96px; text-indent:170.97px;'>79</span>\n  <span style='font-size:9.96px; text-indent:180.94px;'>.</span>\n  <span style='font-size:9.96px; text-indent:183.70px;'>6%</span>\n  <span style='font-size:9.96px; text-indent:196.99px;'>\u2192</span>\n  <span style='font-size:9.96px; text-indent:206.95px;'>84</span>\n  <span style='font-size:9.96px; text-indent:216.91px;'>.</span>\n  <span style='font-size:9.96px; text-indent:219.68px;'>1%</span>\n  <span style='font-size:9.96px; text-indent:232.96px;'>), and D</span>\n  <span style='font-size:6.97px; text-indent:270.27px;'>4</span>\n  <span style='font-size:9.96px; text-indent:274.74px;'>LA</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>(</span>\n  <span style='font-size:9.96px; text-indent:53.43px;'>67</span>\n  <span style='font-size:9.96px; text-indent:63.39px;'>.</span>\n  <span style='font-size:9.96px; text-indent:66.16px;'>7%</span>\n  <span style='font-size:9.96px; text-indent:79.44px;'>\u2192</span>\n  <span style='font-size:9.96px; text-indent:89.41px;'>68</span>\n  <span style='font-size:9.96px; text-indent:99.37px;'>.</span>\n  <span style='font-size:9.96px; text-indent:102.14px;'>8%</span>\n  <span style='font-size:9.96px; text-indent:115.42px;'>). The code and models as well as the D</span>\n  <span style='font-size:6.97px; text-indent:270.27px;'>4</span>\n  <span style='font-size:9.96px; text-indent:274.74px;'>LA</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>dataset will be made publicly available</span>\n  <span style='font-size:6.97px; text-indent:206.47px;'> </span>\n  <span style='font-size:6.97px; text-indent:208.95px;'>1</span>\n  <span style='font-size:9.96px; text-indent:212.94px;'>.</span>\n</plain text>\n<plain text id=entity_5>\n  <span style='font-size:9.96px; text-indent:62.07px;'>Documents are important carriers of human knowledge.</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>With the advancement of digitization, the techniques for au-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>tomatically reading [38, 50, 39, 40, 9], parsing [51, 31],</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>and understanding documents [32, 46, 20, 25] have become</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>a crucial part of the success of digital transformation [8].</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>Document Layout Analysis (DLA) [4], which transforms</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>documents into structured representations, is an essential</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>stage for downstream tasks, such as document retrieval, ta-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>ble extraction, and document information extraction. Tech-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>nically, the goal of DLA is to detect and identify homoge-</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>neous document regions based on visual cues and textual</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>content within the document. However, performing DLA in</span>\n  <span style='font-size:9.96px; text-indent:50.11px;'>real-world scenarios is faced with numerous difficulties: va-</span>\n</plain text>\n<plain text id=entity_6>\n  <span style='font-size:9.96px; text-indent:320.82px;'>Basically, DLA can be regarded as an object detection or</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>semantic segmentation task for document images in com-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>puter vision. Early works [37, 34] directly use visual fea-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>tures encoded by convolutional neural networks (CNN) [19]</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>for layout units detection [36, 30, 35, 17], and have been</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>proven to be effective. Recent years have witnessed the</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>success of document pre-training. Document Image Trans-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>former (DiT) [25] uses images for pre-training, obtaining</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>good performance on DLA. Due to the multi-modal nature</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>of documents, previous methods [15, 20] propose to pre-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>train multi-modal Transformers for document understand-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>ing. However, these methods still employ only visual infor-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>mation for DLA fine-tuning. This might lead to degraded</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>performances and generalization ability for DLA models.</span>\n</plain text>\n<plain text id=entity_7>\n  <span style='font-size:9.96px; text-indent:308.86px;'>riety of document types, complex layouts, low-quality im-</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>ages, semantic understanding, etc. In this sense, DLA is a</span>\n  <span style='font-size:9.96px; text-indent:308.86px;'>very challenging task in practical applications.</span>\n</plain text>\n<plain text id=entity_8>\n  <span style='font-size:11.96px; text-indent:172.18px;'>Cheng Da</span>\n  <span style='font-size:7.97px; text-indent:220.33px;'>*</span>\n  <span style='font-size:11.96px; text-indent:223.32px;'>, Chuwei Luo</span>\n  <span style='font-size:7.97px; text-indent:288.73px;'>\u2217</span>\n  <span style='font-size:11.96px; text-indent:293.47px;'>, Qi Zheng, and Cong Yao</span>\n  <span style='font-size:7.97px; text-indent:418.78px;'>\u2020</span>\n</plain text>\n<plain text id=entity_9>\n  <span style='font-size:20.00px; text-indent:32.00px;'>arXiv:2308.14978v1  [cs.CV]  29 Aug 2023</span>\n</plain text>\n<plain text id=entity_10>\n  <span style='font-size:11.96px; text-indent:172.18px;'>Cheng Da</span>\n  <span style='font-size:7.97px; text-indent:220.33px;'>*</span>\n  <span style='font-size:11.96px; text-indent:223.32px;'>, Chuwei Luo</span>\n  <span style='font-size:7.97px; text-indent:288.73px;'>\u2217</span>\n  <span style='font-size:11.96px; text-indent:293.47px;'>, Qi Zheng, and Cong Yao</span>\n  <span style='font-size:7.97px; text-indent:418.78px;'>\u2020</span>\n  <span style='font-size:11.96px; text-indent:179.03px;'>DAMO Academy, Alibaba Group, Beijing, China</span>\n</plain text>\n<plain text id=entity_11>\n  <span style='font-size:8.97px; text-indent:146.98px;'>dc.dacheng08,luochuwei,zhengqisjtu,yaocong2010@gmail.com</span>\n</plain text>\n<footnote id=entity_12>\n  <span style='font-size:5.98px; text-indent:60.97px;'>1</span>\n  <span style='font-size:7.97px; text-indent:64.46px;'>https://github.com/AlibabaResearch/AdvancedLiterateMachinery</span>\n</footnote>\n<footnote id=entity_13>\n  <span style='font-size:5.98px; text-indent:60.97px;'>*</span>\n  <span style='font-size:7.97px; text-indent:64.46px;'>Equal contribution.</span>\n  <span style='font-size:7.97px; text-indent:125.84px;'> \u2020</span>\n  <span style='font-size:7.97px; text-indent:132.08px;'> Corresponding author.</span>\n</footnote>\n<table id=entity_14>\n  <span style='font-size:8.97px; text-indent:308.86px;'>training techniques with existing SOTA methods in DLA task.</span>\n  <span style='font-size:9.96px; text-indent:352.40px;'>Models</span>\n  <span style='font-size:9.96px; text-indent:429.55px;'>Vision</span>\n  <span style='font-size:9.96px; text-indent:464.28px;'>Text</span>\n  <span style='font-size:9.96px; text-indent:490.70px;'>Pre-trained</span>\n  <span style='font-size:9.96px; text-indent:327.36px;'>CNN-based [37, 34]</span>\n  <span style='font-size:9.96px; text-indent:439.17px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:470.66px;'>\u2717</span>\n  <span style='font-size:9.96px; text-indent:512.39px;'>\u2717</span>\n  <span style='font-size:9.96px; text-indent:338.08px;'>ViT-based [25]</span>\n  <span style='font-size:9.96px; text-indent:439.17px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:470.66px;'>\u2717</span>\n  <span style='font-size:9.96px; text-indent:511.47px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:314.21px;'>Multi-modal PTM [15, 20]</span>\n  <span style='font-size:9.96px; text-indent:439.17px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:470.66px;'>\u2717</span>\n  <span style='font-size:9.96px; text-indent:511.47px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:328.75px;'>Grid-based [45, 47]</span>\n  <span style='font-size:9.96px; text-indent:439.17px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:469.75px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:512.39px;'>\u2717</span>\n  <span style='font-size:9.96px; text-indent:341.89px;'>VGT (Ours)</span>\n  <span style='font-size:9.96px; text-indent:439.17px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:469.75px;'>\u2713</span>\n  <span style='font-size:9.96px; text-indent:511.47px;'>\u2713</span>\n</table>\n<table caption id=entity_15>\n  <span style='font-size:8.97px; text-indent:308.86px;'>Table 1. Comparisons of the use of different modalities and pre-</span>\n  <span style='font-size:8.97px; text-indent:308.86px;'>training techniques with existing SOTA methods in DLA task.</span>\n</table caption>\n"